{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between multi-class and multi-label classification is the choice of the output layer. The usual choice for multi-class classification is the softmax layer. The softmax function is a generalization of the logistic function that “squashes” a K-dimensional vector \\mathbf{z} of arbitrary real values to a K-dimensional vector \\sigma(\\mathbf{z}) of real values in the range [0, 1] that add up to 1.\n",
    "\n",
    "Thus, using the softmax activation function at the output layer results in a neural network that models the probability of a class c_j as multinominal distribution. A consequence of using the softmax function is that the probability for a class is not independent from the other class probabilies. This is nice as long as we only want to predict a single label per sample.\n",
    "\n",
    "This is not the scenario we are looking for, since we want to predict multiple labels at once.\n",
    "Considering the image classification example, the probability that there is a cat in the image should be independent of the probability that there is a dog. Both should be equaly likely.\n",
    "\n",
    "With the sigmoid activation function at the output layer the neural network models the probability of a class c_j as bernoulli distribution (the probability of that specific class be predicted as positive). Therefore, the probabilites of each class is independant from the other class probabilies. So we can use the threshold 0.5 as usual. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Dense(10, activation=\"relu\", input_shape=(10,)))\n",
    "nn.add(Dense(5, activation=\"sigmoid\"))\n",
    "nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this work in keras we need to compile the model. An important choice to make is the loss function. We use the binary_crossentropy loss and not the usual in multi-class classification used categorical_crossentropy loss. This might seem unreasonable, but we want to penalize each output node independantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
