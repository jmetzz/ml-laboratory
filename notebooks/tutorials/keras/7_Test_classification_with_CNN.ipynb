{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM\n",
    "from keras.layers import Conv1D, Flatten, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workaround on ValueError exception when loading pickle file\n",
    "\n",
    "Since the curent version of numpy (1.16.4) sets `allow_pickle` to `False` by default, we need to overwrite this parameter to be able to load the dataset into memory.\n",
    "We should, obviously, reset the default parameters later. See how this is done in the next cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "vocabulary_size = 5000\n",
    "max_len = 1000\n",
    "batch_size = 32\n",
    "embedding_dims= 25\n",
    "filters = 16\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset\n",
    "# tokenizer = text.Tokenizer(num_words=vocabulary_size)\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# X_train = tokenizer.text_to_matrix(X_train)\n",
    "# X_test = tokenizer.text_to_matrix(X_test)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model creating our own embedding with keras\n",
    "\n",
    "model = Sequential()\n",
    "# layer to map the vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(vocabulary_size, embedding_dims, input_length=max_len))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Add a Convolution1D to learn word group filters of size filter_length\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu'))\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D())\n",
    "\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_dims, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# The output layer: positive or negative review\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 76s 3ms/step - loss: 0.4579 - acc: 0.7545 - val_loss: 0.3545 - val_acc: 0.8429\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 77s 3ms/step - loss: 0.3144 - acc: 0.8671 - val_loss: 0.3272 - val_acc: 0.8572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6576aa890>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model using Glove embedding\n",
    "\n",
    "def load_glove_embeddings(src_path):\n",
    "    embeddings_index = dict()\n",
    "    filename = os.path.join(src_path, 'glove.6B.100d.txt')\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        \n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_glove_embeddings('../../../data/non_versioned')\n",
    "embeddings_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulaty_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Make this layer use the Glove embedding and do not update during training\n",
    "model.add(Embedding(vocabulary_size, 100, input_length=max_len, weights=[embeddings_matrix], trainable=False))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "# Add a Convolution1D to learn word group filters of size filter_length\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu'))\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_dims, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# The output layer: positive or negative review\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 162s 6ms/step - loss: 0.6932 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 158s 6ms/step - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x650479590>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-laboratory] *",
   "language": "python",
   "name": "conda-env-ml-laboratory-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
